{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7924190",
   "metadata": {},
   "source": [
    "# PYNQ Board Facial Recognition Test\n",
    "\n",
    "This notebook provides a simple test of the facial recognition system on the PYNQ board.\n",
    "It includes setup, face detection, and recognition from a camera feed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9fdb8",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies\n",
    "\n",
    "First, we need to install the necessary packages. This may take some time on the PYNQ board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70cfd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (only run once)\n",
    "# Import sys to check if we're running the correct Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Uncomment and run this cell if you need to install dependencies\n",
    "# !pip install numpy opencv-python pillow torch==1.13.1 torchvision==0.14.1 facenet-pytorch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c40c2e",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Let's import the necessary libraries for the facial recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Check if GPU is available (it won't be on PYNQ board)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99011d7",
   "metadata": {},
   "source": [
    "## 3. Load Face Recognition Models\n",
    "\n",
    "Now we'll load the MTCNN model for face detection and InceptionResnetV1 for face recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import facenet-pytorch models\n",
    "try:\n",
    "    from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "    print(\"Successfully imported facenet-pytorch models\")\n",
    "    \n",
    "    # Initialize models with lightweight settings for PYNQ\n",
    "    mtcnn = MTCNN(\n",
    "        image_size=160,\n",
    "        margin=0,\n",
    "        min_face_size=40,\n",
    "        thresholds=[0.6, 0.7, 0.9],\n",
    "        factor=0.709,\n",
    "        keep_all=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "    print(\"Models loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    print(\"Please make sure facenet-pytorch is installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92135364",
   "metadata": {},
   "source": [
    "## 4. Define Helper Functions\n",
    "\n",
    "Let's define the helper functions for loading known faces and comparing them with detected faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_known_faces(pickle_path):\n",
    "    \"\"\"Load known face embeddings and names from a pickle file.\"\"\"\n",
    "    if os.path.exists(pickle_path):\n",
    "        try:\n",
    "            with open(pickle_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return np.array(data['embeddings']), data['names']\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading known faces: {e}\")\n",
    "            return np.array([]), []\n",
    "    else:\n",
    "        print(f\"Warning: No known faces file at {pickle_path}\")\n",
    "        return np.array([]), []\n",
    "\n",
    "def compare_faces(face_embedding, known_embeddings, known_names, threshold=0.9):\n",
    "    \"\"\"Compare a face embedding with known embeddings.\"\"\"\n",
    "    if known_embeddings.size == 0:\n",
    "        return \"Unknown\", 1.0\n",
    "    \n",
    "    # Compute Euclidean distances\n",
    "    distances = np.linalg.norm(known_embeddings - face_embedding, axis=1)\n",
    "    min_index = np.argmin(distances)\n",
    "    min_distance = distances[min_index]\n",
    "    \n",
    "    if min_distance < threshold:\n",
    "        return known_names[min_index], min_distance\n",
    "    else:\n",
    "        return \"Unknown\", min_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab3a50",
   "metadata": {},
   "source": [
    "## 5. Load Known Faces\n",
    "\n",
    "Let's load the pre-trained face embeddings from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to known faces pickle file - using current notebook directory by default\n",
    "# If your pickle file is somewhere else on the PYNQ board, update this path\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "PICKLE_PATH = os.path.join(current_dir, 'known_faces.pkl')\n",
    "\n",
    "# If file doesn't exist in current directory, try the data directory\n",
    "if not os.path.exists(PICKLE_PATH):\n",
    "    PICKLE_PATH = os.path.join(current_dir, 'data', 'known_faces.pkl')\n",
    "\n",
    "print(f\"Looking for known faces file at: {PICKLE_PATH}\")\n",
    "\n",
    "# Load known faces\n",
    "known_embeddings, known_names = load_known_faces(PICKLE_PATH)\n",
    "print(f\"Loaded {len(known_names)} known faces\")\n",
    "\n",
    "# Print the names of known people\n",
    "if known_names:\n",
    "    print(\"Known identities:\")\n",
    "    for name in set(known_names):\n",
    "        print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85b41e",
   "metadata": {},
   "source": [
    "## 6. Test Camera Connection\n",
    "\n",
    "Let's check if we can connect to the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e23a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_camera(camera_index=0):\n",
    "    \"\"\"Test if we can connect to the camera at the specified index.\"\"\"\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open camera at index {camera_index}\")\n",
    "        return False\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Could not read frame from camera\")\n",
    "        cap.release()\n",
    "        return False\n",
    "    \n",
    "    print(f\"Successfully connected to camera at index {camera_index}\")\n",
    "    print(f\"Frame dimensions: {frame.shape[1]}x{frame.shape[0]}\")\n",
    "    cap.release()\n",
    "    return True\n",
    "\n",
    "# Try camera indices 0 and 1\n",
    "camera_index = 0\n",
    "if not test_camera(camera_index):\n",
    "    camera_index = 1\n",
    "    test_camera(camera_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255b25c",
   "metadata": {},
   "source": [
    "## 7. Capture and Process a Single Frame\n",
    "\n",
    "Let's capture a single frame from the camera and process it to detect faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cc0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture a single frame and process it\n",
    "def process_single_frame(camera_idx=0):\n",
    "    cap = cv2.VideoCapture(camera_idx)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open camera at index {camera_idx}\")\n",
    "        return\n",
    "    \n",
    "    # Set lower resolution for better performance\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    # Capture frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Could not read frame from camera\")\n",
    "        cap.release()\n",
    "        return\n",
    "    \n",
    "    # Convert BGR to RGB (OpenCV uses BGR, PyTorch uses RGB)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect faces\n",
    "    start_time = time.time()\n",
    "    boxes, _ = mtcnn.detect(rgb_frame)\n",
    "    \n",
    "    # Process each detected face\n",
    "    if boxes is not None:\n",
    "        print(f\"Detected {len(boxes)} face(s)\")\n",
    "        \n",
    "        for box in boxes:\n",
    "            # Get coordinates\n",
    "            x1, y1, x2, y2 = [int(b) for b in box]\n",
    "            \n",
    "            # Make sure box coordinates are within frame\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "            \n",
    "            if x2 > x1 and y2 > y1:  # Valid box\n",
    "                # Extract face\n",
    "                face_img = rgb_frame[y1:y2, x1:x2]\n",
    "                \n",
    "                # Convert to PIL and process\n",
    "                pil_img = Image.fromarray(face_img).resize((160, 160))\n",
    "                \n",
    "                # Get embedding\n",
    "                face_tensor = mtcnn(pil_img).unsqueeze(0).to(device)\n",
    "                if face_tensor is not None:\n",
    "                    with torch.no_grad():\n",
    "                        embedding = resnet(face_tensor).cpu().numpy().flatten()\n",
    "                    \n",
    "                    # Compare with known faces\n",
    "                    name, min_dist = compare_faces(embedding, known_embeddings, known_names)\n",
    "                    print(f\"Recognized: {name} (confidence: {1-min_dist:.2f})\")\n",
    "                    \n",
    "                    # Draw rectangle and name\n",
    "                    color = (0, 255, 0) if name != \"Unknown\" else (0, 0, 255)\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(frame, f\"{name} ({min_dist:.2f})\", (x1, y1 - 10),\n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    else:\n",
    "        print(\"No faces detected\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    print(f\"Processing time: {processing_time:.2f} seconds\")\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Convert BGR to RGB for display in notebook\n",
    "    result_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return result_rgb\n",
    "\n",
    "# Process a single frame from the camera\n",
    "result = process_single_frame(camera_index)\n",
    "\n",
    "# Display the result\n",
    "if result is not None:\n",
    "    from IPython.display import display\n",
    "    from PIL import Image\n",
    "    display(Image.fromarray(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a942fc",
   "metadata": {},
   "source": [
    "## 8. Live Video Processing (Use with Caution)\n",
    "\n",
    "This cell processes live video for a short duration. It might be resource-intensive on the PYNQ board, so use it with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cfc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell might be resource-intensive for the PYNQ board\n",
    "# It stops after a few seconds to prevent overloading\n",
    "\n",
    "def process_video_briefly(camera_idx=0, duration=5, skip_frames=3):\n",
    "    \"\"\"Process video for a short duration, skipping frames for better performance.\n",
    "    \n",
    "    Args:\n",
    "        camera_idx: Camera index (0 or 1)\n",
    "        duration: Duration in seconds to run\n",
    "        skip_frames: Process every Nth frame for better performance\n",
    "    \"\"\"\n",
    "    # Create and configure VideoCapture\n",
    "    cap = cv2.VideoCapture(camera_idx)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open camera at index {camera_idx}\")\n",
    "        return\n",
    "    \n",
    "    # Set resolution for better performance\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)  # Lower resolution for PYNQ\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)  # Lower resolution for PYNQ\n",
    "    \n",
    "    # Create output for video display in notebook\n",
    "    from IPython.display import display, clear_output\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Process frames for specified duration\n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "    try:\n",
    "        while (time.time() - start_time) < duration:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            if frame_count % skip_frames != 0:\n",
    "                continue  # Skip this frame\n",
    "            \n",
    "            # Convert to RGB for processing\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detect faces\n",
    "            try:\n",
    "                boxes, _ = mtcnn.detect(rgb_frame)\n",
    "                \n",
    "                if boxes is not None:\n",
    "                    for box in boxes:\n",
    "                        # Get coordinates\n",
    "                        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "                        \n",
    "                        # Make sure coordinates are valid\n",
    "                        x1, y1 = max(0, x1), max(0, y1)\n",
    "                        x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "                        \n",
    "                        if x2 > x1 and y2 > y1:\n",
    "                            # Extract face\n",
    "                            face_img = rgb_frame[y1:y2, x1:x2]\n",
    "                            \n",
    "                            # Convert to PIL and get embedding\n",
    "                            pil_img = Image.fromarray(face_img).resize((160, 160))\n",
    "                            face_tensor = mtcnn(pil_img).unsqueeze(0).to(device)\n",
    "                            \n",
    "                            if face_tensor is not None:\n",
    "                                with torch.no_grad():\n",
    "                                    embedding = resnet(face_tensor).cpu().numpy().flatten()\n",
    "                                \n",
    "                                # Compare with known faces\n",
    "                                name, min_dist = compare_faces(embedding, known_embeddings, known_names)\n",
    "                                \n",
    "                                # Draw rectangle and name\n",
    "                                color = (0, 255, 0) if name != \"Unknown\" else (255, 0, 0)\n",
    "                                cv2.rectangle(frame, (x1, y1), (x2, y2), color[::-1], 2)  # BGR format for OpenCV\n",
    "                                cv2.putText(frame, f\"{name} ({min_dist:.2f})\", (x1, y1 - 10),\n",
    "                                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color[::-1], 2)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in frame processing: {e}\")\n",
    "            \n",
    "            # Display the result (clear previous output and show new frame)\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            display(plt.gcf())\n",
    "            plt.pause(0.01)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in video processing: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        print(f\"Processed {frame_count // skip_frames} frames over {duration} seconds\")\n",
    "\n",
    "# Uncomment to run live video processing (use with caution on PYNQ board)\n",
    "# process_video_briefly(camera_index, duration=5, skip_frames=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48487e29",
   "metadata": {},
   "source": [
    "## 9. Performance Monitoring\n",
    "\n",
    "Let's test the performance of different components to understand the bottlenecks on the PYNQ board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f70f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_test():\n",
    "    \"\"\"Test the performance of different components.\"\"\"\n",
    "    print(\"Running performance tests...\")\n",
    "    \n",
    "    # Test face detection\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Could not read frame from camera\")\n",
    "        return\n",
    "    \n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Test MTCNN detection performance\n",
    "    start_time = time.time()\n",
    "    boxes, _ = mtcnn.detect(rgb_frame)\n",
    "    mtcnn_time = time.time() - start_time\n",
    "    print(f\"MTCNN face detection: {mtcnn_time:.4f} seconds\")\n",
    "    \n",
    "    # Skip face recognition if no faces detected\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        print(\"No faces detected for recognition test\")\n",
    "        return\n",
    "    \n",
    "    # Extract a face for recognition test\n",
    "    x1, y1, x2, y2 = [int(b) for b in boxes[0]]\n",
    "    face_img = rgb_frame[y1:y2, x1:x2]\n",
    "    pil_img = Image.fromarray(face_img).resize((160, 160))\n",
    "    \n",
    "    # Test face embedding generation\n",
    "    start_time = time.time()\n",
    "    face_tensor = mtcnn(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = resnet(face_tensor)\n",
    "    embedding_time = time.time() - start_time\n",
    "    print(f\"Face embedding generation: {embedding_time:.4f} seconds\")\n",
    "    \n",
    "    # Test face comparison\n",
    "    start_time = time.time()\n",
    "    compare_faces(embedding.cpu().numpy().flatten(), known_embeddings, known_names)\n",
    "    comparison_time = time.time() - start_time\n",
    "    print(f\"Face comparison: {comparison_time:.4f} seconds\")\n",
    "    \n",
    "    # Total time for one face processing\n",
    "    total_time = mtcnn_time + embedding_time + comparison_time\n",
    "    print(f\"Total processing time for one face: {total_time:.4f} seconds\")\n",
    "    print(f\"Theoretical max FPS: {1/total_time:.2f}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_info = process.memory_info()\n",
    "        print(f\"Memory usage: {memory_info.rss / (1024 * 1024):.2f} MB\")\n",
    "    except ImportError:\n",
    "        print(\"psutil not available, skipping memory usage info\")\n",
    "\n",
    "# Run performance tests\n",
    "try:\n",
    "    performance_test()\n",
    "except Exception as e:\n",
    "    print(f\"Error during performance test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d284d38",
   "metadata": {},
   "source": [
    "## 10. Cleanup\n",
    "\n",
    "Let's clean up to release resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "import gc\n",
    "\n",
    "# Delete large objects\n",
    "del mtcnn\n",
    "del resnet\n",
    "del known_embeddings\n",
    "\n",
    "# Run garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# If using CUDA (not on PYNQ)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Resources cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
